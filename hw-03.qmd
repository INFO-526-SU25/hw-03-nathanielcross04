---
title: "HW 03"
subtitle: "INFO 526 Summer 2025"
author: "Nathaniel Cross"
format:
  html:
    embed-resources: true
toc: true
---

## 0 - Setup

```{r}
#| label: R Setup

# Install and load packages
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(tidyverse, 
               glue,
               scales,
               countdown,
               ggthemes,
               gt,
               palmerpenguins,
               openintro,
               ggrepel,
               patchwork,
               quantreg,
               janitor,
               colorspace,
               broom,
               fs,
               here,
               openintro,
               gghighlight,
               lubridate,
               dsbox,
               ggridges,
               gtable,
               ggimage,
               png,
               ggpubr
               )

devtools::install_github("tidyverse/dsbox")

# Set theme for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 14))

# Set width of code output
options(width = 65)

# Set figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 7,        # 7" width
  fig.asp = 0.618,      # the golden ratio
  fig.retina = 3,       # dpi multiplier for displaying HTML output on retina
  fig.align = "center", # center align figures
  dpi = 300             # higher dpi, sharper image
)
```

## 1 - Du Bois challenge.

```{r}
#| label: Question 1 code

# Loading in the data
income <- read_csv("data/income.csv")

income |>
  glimpse()

# Correcting the data
income[1, 6] = 0.1
income[1, 7] = 9.9 # Source: Marcel Hebing, StackOverflow

# Pivoting the data long
income_clean <- income |>
  pivot_longer(
    cols = c("Rent", "Food", "Clothes", "Tax", "Other"), 
    names_to = "category", 
    values_to = "expenditure"
  ) |>
  glimpse()

# Set image
image <- "https://cdn.pixabay.com/photo/2012/12/06/06/27/paper-68829_1280.jpg"

# Releveling factor variables
income_clean <- income_clean |>
  mutate(
    Class = fct_relevel(Class, "$100-200", "$200-300", "$300-400", "$400-500", "$500-750", "$750-1000", "$1000 AND OVER"),
    category = fct_relevel(category, "Rent", "Food", "Clothes", "Tax", "Other"),
    Class = fct_rev(Class),
    category = fct_rev(category)
  )

# Creating cumulative summations
income_clean <- income_clean |>
  group_by(Class) |>
  mutate(label_y = cumsum(expenditure) - 0.5 * expenditure) |>
  ungroup() # Source: R Graphics Codebook


# Prepping labels
income_clean2 <- income_clean|>
  filter(category != "Rent" & category != "Tax")

income_clean2 <- income_clean2 |>
  mutate(
    perc = "%",
    expend2 = glue("{expenditure}{perc}")
  ) 
  
# Plotting
plot <- income_clean |>
  ggplot(aes(x = Class, y = expenditure, fill = category)) +
  geom_col(width = 0.5) +# Source: Geeks for Geeks (https://www.geeksforgeeks.org/r-language/grouped-stacked-and-percent-stacked-barplot-in-ggplot2/) +
    geom_text(data = income_clean2, aes(y = label_y, label = expend2), color = "black", size = 3, family = "mono") + 
  labs(
    y = NULL,
    x = NULL,
    title = "INCOME AND EXPENDITURE OF 150 NEGRO FAMILIES IN ATLANTA, GA., U.S.A."
  ) +
   annotate(
    geom = "text",
    x = c("$100-200", "$200-300", "$300-400", "$400-500", "$500-750"),
    y = c(9.5, 11, 11.5, 9, 6.5),
    label = c("19%", "22%", "23%", "18%", "13%"),
    size = 3,
    color = "white",
    family = "mono"
  ) +
   annotate(
    geom = "text",
    x = c("$200-300", "$300-400", "$400-500", "$500-750", "$750-1000", "$1000 AND OVER"),
    y = c(94, 86.25, 72.75, 63.5, 60, 47.25),
    label = c("4%", "4.5%", "5.5%", "5%", "8%", "4.5%"),
    size = 2,
    color = "black",
    family = "mono"
  ) +
   scale_x_discrete(labels = c("1,000     $1,125 \nAND OVER          ", "$750-1000   $880   ", "$500-750   $547   ", "$400-500   $433.82", "$300-400   $335.66", "$200-300   $249.45", "$100-200   $139.10")) +
  coord_flip(clip = "off") +
  scale_fill_manual(values = c("snow2", "slategray1", "sienna1", "purple", "black")) +
  theme(
    legend.position = "top",
    plot.title.position = "plot",
    plot.title = element_text(size = 10, hjust = 0.5, face = "bold"),
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(size =7),
    text = element_text(family = "mono"),
    legend.title = element_blank(), 
    legend.text  = element_text(size = 9),
    legend.key.size = unit(0.3, "cm") # Source: https://www.tidyverse.org/blog/2024/02/ggplot2-3-5-0-legends/
  )
  
ggbackground(plot, image) # Source: Guangchuang Yu (https://guangchuangyu.github.io/2018/04/setting-ggplot2-background-with-ggbackground/)
```

## 2 - COVID survey - interpret

The visualization deconstructs responses to six statements regarding perceptions of the COVID-19 vaccine and its safety. Responses are measured on a 1-5 Likert scale, with the means depicted in the figure. Error bars are also depicted from the 10th to 90th percentiles of responses per question. Responses are shown on a macro-level first, then classified by identity, such as age, gender, and race (among others). For each statement, there is minor variation by identity group in mean response on the Likert scale; the interesting part of this figure is derived from its depiction of the distributions of each response, which are far less homogeneous. As the surveyed respondents are nursing and medical university students across the United States, most responses indicate high trust in the vaccine and its safety. This holds true for the statements, "I believe the vaccine is safe," "Getting the vaccine makes me feel safer at work," "I am confident in the scientific vetting process for new vaccines," "I trust the information I have received," and "I will recommend the vaccine to others" (abridged for brevity), where all respondents express agreement with the statement (strong or somewhat agree). Less agreement is introduced in response to the statement, "I am concerned about side effects," where agreement is neutral (neither agree nor disagree).

Some of the most intriguing results for me in this plot are located among identity groups with heterogeneous variation in responses between statements. For example, there is high variation in responses to the statement "I believe the vaccine is safe" among Asians (with the 10-90th percentiles encompassing almost the entire scale), however this variation is almost completely eliminated when the same group was asked if they would recommend the vaccine to others. I would have expected similar variation in response to these questions because if one does not find the vaccine safe, I would expect them to be less likely to recommend it. I also find it interesting in comparing variation between statements at the macro level, regardless of identity group. More particularly, in almost every identity group there is high heterogeneity in responses to the "I am concerned about side effects" statement, however much less variation in "I will recommend the vaccine to others" statement. This could be indicative of a stronger sense of community in light of a global pandemic (dependent on when the data were collected). Finally, I find a comparison of nursing and medical students interesting, especially when comparing the distributions of their responses. As both are in the medical field literally learning about the processes involved in designing and selling vaccines, I would have expected the distribution of these responses to be similar, however for all statements but the first and third, the distributions are larger for medical students than nurses. While this difference may not be statistically significant, is makes an interesting comparison and begs the questions as to why.

## 3 - COVID survey - reconstruct

```{r}
#| label: Question 3 code

# Loading in data
covid <- read_csv("data/covid-survey.csv")

# Renaming columns to row 1
colnames(covid) <- covid[1,] # Source: https://stackoverflow.com/questions/32054368/use-first-row-data-as-column-names-in-r

# Deleting row 1 (now column names)
remove.rows <- 1
covid <- covid[!(row.names(covid) %in% remove.rows),] # Source: https://www.tutorialspoint.com/how-to-remove-rows-in-an-r-data-frame-using-row-names

# Dimensions of dataset
dim(covid)

# Eliminate rows with all NAs but response ID
covid <- covid |>
  filter(!if_all(-response_id, is.na))

# Dimensions of the dataset
dim(covid)

# Relabel values of columns
covid |>
  glimpse()

covid <- covid |>
    mutate(
      exp_already_vax = recode(exp_already_vax, "0" = 'No', "1" = 'Yes'),      
      exp_flu_vax = recode(exp_flu_vax, "0" = 'No', "1" = 'Yes'),
      exp_profession = recode(exp_profession, "0" = 'Medical', "1" = 'Nursing'),
      exp_gender = recode(exp_gender, "0" = 'Male', "1" = 'Female', "3" = 'Non-binary third gender', "4" = 'Prefer not to say'),
      exp_race = recode(exp_race, "1" = 'American Indian/Alaskan Native', "2" = 'Asian', "3" = 'Black/African American', "4" = 'Native Hawaiian/Other Pacific Islander', "5" = 'White'),
      exp_ethnicity = recode(exp_ethnicity, "1" = 'Hispanic/Latino', "2" = 'Non-Hispanic/Non-Latino'),
      exp_age_bin = recode(exp_age_bin, "0" = '<20', "20" = '21-25', "25" = '26-30', "30" = '>30')
    )

# Dimensions of dataset
dim(covid)

# Calculating mean and percentiles
covid_survey_longer <- covid |>
  pivot_longer(
    cols = starts_with("exp_"),
    names_to = "explanatory",
    values_to = "explanatory_value"
  ) |> # This pivot lengthens the data so that instead of one column for each explanatory variable, there are instead multiple observations of each response, one per explanatory variable. The values of these explanatory variables were pivoted into a new column, explanatory_value.
  filter(!is.na(explanatory_value)) |> # Eliminates NAs from explanatory values.
  pivot_longer(
    cols = starts_with("resp_"),
    names_to = "response",
    values_to = "response_value"
  ) # This pivot again lengthens the data, this time for response variables. This means that for each response variable for each response ID, explanatory variables and their values are listed.

# Display tibble
covid_survey_longer
```

## 4 - COVID survey - re-reconstruct

## 5 - COVID survey - another view
